Clone
-----

- Clone the repo:
	git clone --recurse-submodules HOSTURL:WORKSPACE/gpt_batch_api_build.git
	cd gpt_batch_api_build

- Initialize local config:
	git config --local submodule.recurse true
	git config --local push.recurseSubmodules on-demand
	git config --local diff.submodule log
	git config --local status.submodulesummary true
	git config --local alias.sdiff '!'"git diff && git submodule foreach 'git diff'"
	git config --local alias.supdate 'submodule update --remote --rebase'

- Update an existing clone:
	git pull --rebase
	git submodule sync --recursive
	git submodule update --init --recursive

- Check out a particular release (e.g. version 1.0.1):
	git checkout v1.0.1
	git submodule status

Build
-----

- Make sure the git state is clean in both gpt_batch_api and gpt_batch_api_build (commit and push to all remotes as required)

- Update any version numbers if required (search-replace for the old version number)
- Create new tags in both gpt_batch_api and gpt_batch_api_build for the new version number:
	git tag -a v1.0.1 -m "Release version 1.0.1"
	git push origin --tags  # <-- For each remote...

- Get/activate (e.g. in a venv or conda) the minimum Python version to be supported by the package (currently Python 3.12)
- Make sure this Python version has 'build' installed:
	pip install build

- Ensure the dist/egg-info directories are clean/removed (ignore build/, if it exists)
- Build the package (in the directory with pyproject.toml):
	python -m build
- Examine the produced output files:
	dist/gpt_batch_api-*.tar.gz
	dist/gpt_batch_api-*-py3-none-any.whl
	gpt_batch_api.egg-info/*

- Test a local installation of the wheel:
	conda create -n gpt_batch_api_test python=3.12
	conda activate gpt_batch_api_test
	pip install dist/gpt_batch_api-*-py3-none-any.whl
	cd ~  # <-- Just to be sure there is no gpt_batch_api directory on the path...
	python
		import gpt_batch_api
		print(gpt_batch_api.__version__)                              # Version
		print(gpt_batch_api.TaskManager, gpt_batch_api.GPTRequester)  # Two main library classes
		import os
		print(os.path.join(os.path.dirname(gpt_batch_api.__file__), 'commands.txt'))  # Location of the installed commands.txt file (refer to this for command/script help)

- Test running scripts:
	python -m gpt_batch_api.task_manager_demo --help
	python -m gpt_batch_api.wandb_configure_view --help

- Test running a script that actually makes API calls (requires less than 0.01 USD):
	export OPENAI_API_KEY=sk-...  # <-- Set the OpenAI API key
	export WANDB_API_KEY=...      # <-- Set the wandb API key (a project called gpt_batch_api is created/used and can be used to monitor the following run in real-time)
	python -m gpt_batch_api.task_manager_demo --task_dir /tmp/gpt_batch_api_tasks --task utterance_emotion --model gpt-4o-mini-2024-07-18 --cost_input_direct_mtoken 0.150 --cost_input_cached_mtoken 0.075 --cost_input_batch_mtoken 0.075 --cost_output_direct_mtoken 0.600 --cost_output_batch_mtoken 0.300 --min_batch_requests 200 --max_direct_requests 40  # <-- The last two arguments avoid actually using the Batch API (as this can take a while to complete, and this is just a quick test)
	python -m gpt_batch_api.wandb_configure_view --dst_entity ENTITY  # <-- [Substitute correct ENTITY! / Only need to execute this once ever per project!] Then go to https://wandb.ai/ENTITY/gpt_batch_api and select the saved view called 'GPT Batch API', and then click 'Copy to my workspace'
	# Output files: Refer to /tmp/gpt_batch_api_tasks directory
	# Note: If task_dir is not specified then a tasks directory will be auto-created inside the installed site-packages location, which is probably not desired in general

PyPi Release
------------

- Get/activate a Python version and ensure it has 'twine' installed:
	pip install twine

- Upload the wheel and source distribution (PYPI_API_TOKEN is pypi-...):
	twine upload --username __token__ --password PYPI_API_TOKEN dist/*

- View result:
	https://pypi.org/project/gpt-batch-api
