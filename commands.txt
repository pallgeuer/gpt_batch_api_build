Clone
-----

- Clone the repo:
	git clone --recurse-submodules HOSTURL:WORKSPACE/gpt_batch_api_build.git
	cd gpt_batch_api_build

- Initialize local config:
	git config --local submodule.recurse true
	git config --local push.recurseSubmodules on-demand
	git config --local diff.submodule log
	git config --local status.submodulesummary true
	git config --local alias.sdiff '!'"git diff && git submodule foreach 'git diff'"
	git config --local alias.supdate 'submodule update --remote --rebase'

- Update an existing clone:
	git pull --rebase
	git submodule sync --recursive
	git submodule update --init --recursive

- Check out a particular release (e.g. version 1.0.5):
	git checkout v1.0.5
	git submodule status

Build
-----

- Make sure the git state is clean in both gpt_batch_api (first) and gpt_batch_api_build (second) => Commit and push to all remotes as required

- Update any version numbers if required (search-replace for the old version number and replace it with the new version number)
- Commit and push gpt_batch_api (first) and gpt_batch_api_build (second) to all remotes:
	git commit -a -m "Version 1.0.5"
	git push origin  # <-- For each remote...

- Create new tags in both gpt_batch_api and gpt_batch_api_build for the new version number:
	git tag -a v1.0.5 -m "Release version 1.0.5"
	git push origin --tags  # <-- For each remote...

- Get/activate (e.g. in a venv or conda, e.g. conda activate gpt_batch_api) the minimum Python version to be supported by the package (currently Python 3.12)
- Make sure this Python version has 'build' installed:
	pip install build

- Ensure the dist/egg-info directories are clean/removed (ignore build/ if it exists)
- Build the package (in the directory with pyproject.toml):
	python -m build
- Examine the produced output files:
	dist/gpt_batch_api-*.tar.gz
	dist/gpt_batch_api-*-py3-none-any.whl
	gpt_batch_api.egg-info/*

- Test a local installation of the wheel:
	conda create -n gpt_batch_api_test python=3.12
	conda activate gpt_batch_api_test
	pip install dist/gpt_batch_api-*-py3-none-any.whl
	cd ~  # <-- Just to be sure there is no gpt_batch_api directory on the path...
	python
		import gpt_batch_api
		print(gpt_batch_api.__version__)                              # Version
		print(gpt_batch_api.TaskManager, gpt_batch_api.GPTRequester)  # Two main library classes

- Test running scripts:
	python -m gpt_batch_api.task_manager_demo --help
	python -m gpt_batch_api.wandb_configure_view --help

- Test running a script that actually makes API calls (requires less than 0.01 USD):
	export OPENAI_API_KEY=sk-...  # <-- Set the OpenAI API key
	export WANDB_API_KEY=...      # <-- Set the wandb API key (a project called gpt_batch_api is created/used and can be used to monitor the following run in real-time)
	python -m gpt_batch_api.task_manager_demo --task_dir /tmp/gpt_batch_api_tasks --task utterance_emotion --model gpt-4o-mini-2024-07-18 --cost_input_direct_mtoken 0.150 --cost_input_cached_mtoken 0.075 --cost_input_batch_mtoken 0.075 --cost_output_direct_mtoken 0.600 --cost_output_batch_mtoken 0.300 --force_direct  # <-- The last argument actually avoids use of the Batch API and forces use of the direct API instead (as the batch API can take a while to complete and this command should just be a quick test)
	python -m gpt_batch_api.wandb_configure_view --dst_entity ENTITY  # <-- [Substitute correct ENTITY! / Only need to execute this once ever per project!] Then go to https://wandb.ai/ENTITY/gpt_batch_api and select the saved view called 'GPT Batch API', and then click 'Copy to my workspace'
	# Output files: Refer to /tmp/gpt_batch_api_tasks directory
	# Note: If task_dir is not specified then a tasks directory will be auto-created inside the installed site-packages location, which is probably not desired in general

- Remove the test conda environment:
	conda env remove -n gpt_batch_api_test
	cd -  # <-- Revert the cd ~ from above

PyPi Release
------------

- Get/activate a Python version and ensure it has 'twine' installed:
	pip install twine

- Upload the wheel and source distribution:
	PYPI_API_TOKEN=pypi-...  # <-- CAUTION: Set correct PyPi API token (consider adding a leading space to avoid the command being saved to the bash history)
	twine upload --username __token__ --password "$PYPI_API_TOKEN" dist/*

- View result:
	https://pypi.org/project/gpt-batch-api
